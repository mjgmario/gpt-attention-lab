# =============================================================================
# Attention Lab - Default Configuration
# =============================================================================
# Override by creating config/local.yaml or passing --config path/to/config.yaml

# -----------------------------------------------------------------------------
# Model Configuration
# -----------------------------------------------------------------------------
model:
  vocab_size: null             # Auto-detect from dataset
  block_size: 128              # Maximum context length
  n_layer: 4                   # Number of transformer blocks
  n_head: 4                    # Number of attention heads
  n_embd: 128                  # Embedding dimension
  dropout: 0.1                 # Dropout probabi lity
  bias: false                  # Use bias in linear layers

# -----------------------------------------------------------------------------
# Attention Variant
# -----------------------------------------------------------------------------
# Options: vanilla, linear, sliding_window, sparse, rotary
attention:
  type: vanilla

  # Variant-specific settings
  sliding_window:
    window_size: 16

  sparse:
    local_size: 16
    stride: 16

# -----------------------------------------------------------------------------
# Training Configuration
# -----------------------------------------------------------------------------
training:
  max_steps: 5000              # Total training steps
  batch_size: 32               # Batch size
  learning_rate: 3.0e-4        # Learning rate for AdamW
  weight_decay: 0.1            # Weight decay
  grad_clip: 1.0               # Gradient clipping norm
  warmup_steps: 100            # Learning rate warmup

  # Evaluation
  eval_interval: 500           # Steps between evaluations
  eval_steps: 50               # Number of eval batches

  # Logging
  log_interval: 10             # Steps between logging

  # Checkpoints
  checkpoint_dir: checkpoints
  save_best: true              # Save best model by val loss

  # Performance
  compile: false               # Use torch.compile (faster, needs warmup)

# -----------------------------------------------------------------------------
# Data Configuration
# -----------------------------------------------------------------------------
data:
  strategy: weighted
  num_samples: 20000

  # Cache: load preloaded data if available, generate/download if not
  # Run `uv run python scripts/prepare_data.py` to preload
  use_cache: false
  # cache_dir: data/.cache  # custom cache directory (default: data/.cache)

  # Datasets to use
  datasets:
    - type: shakespeare
      weight: 1.0

# -----------------------------------------------------------------------------
# Dataset-specific defaults
# -----------------------------------------------------------------------------
dataset_defaults:
  shakespeare:
    block_size: 128
    train_ratio: 0.9
    # max_samples: null          # Limit dataset size (null = full dataset)

# -----------------------------------------------------------------------------
# Device & Performance
# -----------------------------------------------------------------------------
device: null               # null = auto-detect (cuda > mps > cpu)
num_workers: 0             # DataLoader workers
pin_memory: true           # Pin memory for GPU

# -----------------------------------------------------------------------------
# Experiment Tracking
# -----------------------------------------------------------------------------
experiment:
  name: null               # Experiment name (auto-generated if null)
  seed: 42                 # Random seed for reproducibility
  output_dir: outputs      # Output directory for results

# -----------------------------------------------------------------------------
# Generation (for inference)
# -----------------------------------------------------------------------------
generation:
  max_new_tokens: 100
  temperature: 0.8
  top_k: 40
  top_p: null              # null = disabled
